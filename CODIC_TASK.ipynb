{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing librarys\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ներկայացնում եմ առաջադրանքի կատարումը։ Օգտագործել եմ տարբեր մեթոդներ, նպատակը՝ գտնել ամենա հարմարը հիպերպարամետրերը փորձարկելու և մոդելը ձևափոխելու համար։ Ցավոք, պատերազմական դրության պատճառով մինչև դեդլայնը շատ քիչ ժամանակ է հնարավոր եղել տրամադրել, գումարած, ամեն պարապմունքը մեծ չափանի դատայի վրա իմ մոտ տևում է ժամեր, այդ իսկ պատճառով հիպերպարամետրերը չեմ հասցրել փոփոխել, շերտերի անհրաժեշտ քանակը և կազմությունը հնարավոր չէր որոշել։"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "                            gpus[0],\n",
    "                            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304, 36)\n",
      "(89, 36)\n",
      "(66, 36)\n"
     ]
    }
   ],
   "source": [
    "# First and simple method\n",
    "\n",
    "df = pd.read_csv('train_curated'+'.csv', encoding='utf-8')\n",
    "target = df.pop('labels')\n",
    "\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "label_names = []\n",
    "for row in list(set(target.values)):\n",
    "    label_names.extend(tokenizer.tokenize(row))\n",
    "label_names = list(set(label_names)) # list of labels\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(('train_curated/'+df.values, target.values))\n",
    "\n",
    "def DataPreproc(filename_tensor, labels):\n",
    "    \n",
    "    text = filename_tensor.numpy()[0]\n",
    "    labels = labels.numpy()\n",
    "    y, sr = librosa.load(text, mono = True)\n",
    "\n",
    "    if y.shape[0] < sr:\n",
    "        y = np.pad(y, (0, sr-y.shape[0]), constant_values=(0, -1))\n",
    "    \n",
    "    # Extracting features\n",
    "    fet = [librosa.feature.chroma_stft(y=y, sr=sr), \n",
    "           librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "           librosa.feature.spectral_bandwidth(y=y, sr=sr),\n",
    "           librosa.feature.spectral_rolloff(y=y, sr=sr),\n",
    "           librosa.feature.zero_crossing_rate(y),\n",
    "           librosa.feature.mfcc(y=y, sr=sr)]\n",
    "    \n",
    "    #Preprocessing feature vectors and matrixs to matrixs\n",
    "    features = fet[0]\n",
    "    for i in fet[1:]:\n",
    "        features = np.append(features, i, axis=0)\n",
    "    features = features.T\n",
    "    \n",
    "    # Text labels to int vector\n",
    "    labels = tokenizer.tokenize(labels)\n",
    "    label = np.zeros_like(label_names, dtype=np.int32)\n",
    "    for lb in labels:\n",
    "        label[label_names.index(lb)] = 1\n",
    "        \n",
    "    return features, label\n",
    "\n",
    "def DataPreproc_map(filename_tensor, labels):\n",
    "    return tf.py_function(DataPreproc, inp = [filename_tensor, labels],\n",
    "                          Tout = (tf.float32, tf.int32))\n",
    "\n",
    "EPOCHS = 10 # Epochs for training\n",
    "BATCH_SIZE = 30 # Batch size of training and validation data\n",
    "BUFFER = 3000 # Buffer for shuffling\n",
    "\n",
    "#train_data = train_data.shuffle(BUFFER, reshuffle_each_iteration=False)\n",
    "\n",
    "train_data = train_data.map(DataPreproc_map)\n",
    "\n",
    "test_data = train_data.skip(3200)\n",
    "valid_data = test_data.take(1000)\n",
    "test_data = test_data.skip(1000)\n",
    "train_data = train_data.take(3200)\n",
    "\n",
    "for ex in train_data.take(3):\n",
    "    print(ex[0].shape)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([None, None], [None]))\n",
    "valid_data = valid_data.padded_batch(BATCH_SIZE, padded_shapes=([None, None], [None]))\n",
    "test_data = test_data.batch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidir-lstm (Bidirectional)   (None, 128)               51712     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 86)                44118     \n",
      "=================================================================\n",
      "Total params: 424,534\n",
      "Trainable params: 424,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for ex in train_data.take(1):\n",
    "    shape_1 = ex[0].shape[2]\n",
    "    output_shape = ex[1].shape[1]\n",
    "\n",
    "# Building LSTM model\n",
    "# LSTM layer - in bidirectional, for first - (1,64)\n",
    "\n",
    "bi_lstm_model_1 = tf.keras.Sequential([\n",
    "    layers.Bidirectional(layers.LSTM(512, name = 'lstm-1'), input_shape=(None, shape_1),\n",
    "                                  name = 'bidir-lstm'),\n",
    "    #layers.Bidirectional(layers.LSTM(64, name = 'lstm-2'),\n",
    "     #                             name = 'bidir-lstm_2'),\n",
    "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "    #tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    \n",
    "    tf.keras.layers.Dense(output_shape, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "bi_lstm_model_1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  1/107 [..............................] - ETA: 0s - loss: 0.6891 - accuracy: 0.0333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\librosa\\core\\pitch.py:153: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn(\"Trying to estimate tuning from empty frequency set.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 2833s 26s/step - loss: 0.1141 - accuracy: 0.0134 - val_loss: 0.0761 - val_accuracy: 0.0130\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - 2810s 26s/step - loss: 0.0746 - accuracy: 0.0434 - val_loss: 0.0742 - val_accuracy: 0.0320\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - 2887s 27s/step - loss: 0.0724 - accuracy: 0.0572 - val_loss: 0.0727 - val_accuracy: 0.0430\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.0728 "
     ]
    }
   ],
   "source": [
    "#Compile and train the model\n",
    "# Use Adam optimizer, regularization parameter - None (for first), loss function \n",
    "#- binarycrossentropy, for cumputing loss for each label more effectively\n",
    "\n",
    "bi_lstm_model_1.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                      loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                      metrics = ['accuracy'])\n",
    "history = bi_lstm_model_1.fit(train_data, \n",
    "                            validation_data = valid_data,\n",
    "                            epochs = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "\n",
    "test_results = bi_lstm_model_1.evaluate(test_data)\n",
    "print('Test acc.: {:.2f}%'.format(test_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304, 36)\n",
      "(89, 36)\n",
      "(66, 36)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidir-lstm (Bidirectional)   (None, 256)               168960    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 86)                44118     \n",
      "=================================================================\n",
      "Total params: 607,318\n",
      "Trainable params: 607,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "  1/107 [..............................] - ETA: 0s - loss: 0.6908 - binary_accuracy: 0.0260 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\librosa\\core\\pitch.py:153: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn(\"Trying to estimate tuning from empty frequency set.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 2980s 28s/step - loss: 0.1102 - binary_accuracy: 0.9564 - accuracy: 0.0247 - val_loss: 0.0753 - val_binary_accuracy: 0.9852 - val_accuracy: 0.0510\n",
      "Epoch 2/5\n",
      "107/107 [==============================] - 3109s 29s/step - loss: 0.0733 - binary_accuracy: 0.9855 - accuracy: 0.0566 - val_loss: 0.0722 - val_binary_accuracy: 0.9852 - val_accuracy: 0.0840\n",
      "Epoch 3/5\n",
      "107/107 [==============================] - 3315s 31s/step - loss: 0.0711 - binary_accuracy: 0.9855 - accuracy: 0.0744 - val_loss: 0.0711 - val_binary_accuracy: 0.9852 - val_accuracy: 0.0860\n",
      "Epoch 4/5\n",
      "107/107 [==============================] - 2998s 28s/step - loss: 0.0694 - binary_accuracy: 0.9855 - accuracy: 0.0831 - val_loss: 0.0703 - val_binary_accuracy: 0.9852 - val_accuracy: 0.0960\n",
      "Epoch 5/5\n",
      "107/107 [==============================] - 3434s 32s/step - loss: 0.0681 - binary_accuracy: 0.9855 - accuracy: 0.0934 - val_loss: 0.0697 - val_binary_accuracy: 0.9852 - val_accuracy: 0.0990\n"
     ]
    }
   ],
   "source": [
    "# Model 1_2 version\n",
    "\n",
    "# First and simple method\n",
    "\n",
    "df = pd.read_csv('train_curated'+'.csv', encoding='utf-8')\n",
    "target = df.pop('labels')\n",
    "\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "label_names = []\n",
    "for row in list(set(target.values)):\n",
    "    label_names.extend(tokenizer.tokenize(row))\n",
    "label_names = list(set(label_names)) # list of labels\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(('train_curated/'+df.values, target.values))\n",
    "\n",
    "def DataPreproc(filename_tensor, labels):\n",
    "    \n",
    "    text = filename_tensor.numpy()[0]\n",
    "    labels = labels.numpy()\n",
    "    y, sr = librosa.load(text, mono = True)\n",
    "\n",
    "    if y.shape[0] < sr:\n",
    "        y = np.pad(y, (0, sr-y.shape[0]), constant_values=(0, -1))\n",
    "    \n",
    "    # Extracting features\n",
    "    fet = [librosa.feature.chroma_stft(y=y, sr=sr), \n",
    "           librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "           librosa.feature.spectral_bandwidth(y=y, sr=sr),\n",
    "           librosa.feature.spectral_rolloff(y=y, sr=sr),\n",
    "           librosa.feature.zero_crossing_rate(y),\n",
    "           librosa.feature.mfcc(y=y, sr=sr)]\n",
    "    \n",
    "    #Preprocessing feature vectors and matrixs to matrixs\n",
    "    features = fet[0]\n",
    "    for i in fet[1:]:\n",
    "        features = np.append(features, i, axis=0)\n",
    "    features = features.T\n",
    "    \n",
    "    # Text labels to int vector\n",
    "    labels = tokenizer.tokenize(labels)\n",
    "    label = np.zeros_like(label_names, dtype=np.int32)\n",
    "    for lb in labels:\n",
    "        label[label_names.index(lb)] = 1\n",
    "        \n",
    "    return features, label\n",
    "\n",
    "def DataPreproc_map(filename_tensor, labels):\n",
    "    return tf.py_function(DataPreproc, inp = [filename_tensor, labels],\n",
    "                          Tout = (tf.float32, tf.int32))\n",
    "\n",
    "EPOCHS = 5 # Epochs for training\n",
    "BATCH_SIZE = 30 # Batch size of training and validation data\n",
    "BUFFER = 3000 # Buffer for shuffling\n",
    "\n",
    "#train_data = train_data.shuffle(BUFFER, reshuffle_each_iteration=False)\n",
    "\n",
    "train_data = train_data.map(DataPreproc_map)\n",
    "\n",
    "test_data = train_data.skip(3200)\n",
    "valid_data = test_data.take(1000)\n",
    "test_data = test_data.skip(1000)\n",
    "train_data = train_data.take(3200)\n",
    "\n",
    "for ex in train_data.take(3):\n",
    "    print(ex[0].shape)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([None, None], [None]))\n",
    "valid_data = valid_data.padded_batch(BATCH_SIZE, padded_shapes=([None, None], [None]))\n",
    "test_data = test_data.batch(1)\n",
    "\n",
    "for ex in train_data.take(1):\n",
    "    shape_1 = ex[0].shape[2]\n",
    "    output_shape = ex[1].shape[1]\n",
    "\n",
    "# Building LSTM model\n",
    "# LSTM layer - in bidirectional, for first - (1,64)\n",
    "\n",
    "bi_lstm_model_1 = tf.keras.Sequential([\n",
    "    layers.Bidirectional(layers.LSTM(128, name = 'lstm-1'), input_shape=(None, shape_1),\n",
    "                                  name = 'bidir-lstm'),\n",
    "    #layers.Bidirectional(layers.LSTM(64, name = 'lstm-2'),\n",
    "     #                             name = 'bidir-lstm_2'),\n",
    "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "    #tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    \n",
    "    tf.keras.layers.Dense(output_shape, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "bi_lstm_model_1.summary()\n",
    "\n",
    "#Compile and train the model\n",
    "# Use Adam optimizer, regularization parameter - None (for first), loss function \n",
    "#- binarycrossentropy, for cumputing loss for each label more effectively\n",
    "\n",
    "bi_lstm_model_1.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                      loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                      metrics = [tf.keras.metrics.BinaryAccuracy(threshold=0.4), 'accuracy'])\n",
    "history = bi_lstm_model_1.fit(train_data, \n",
    "                              validation_data = valid_data,\n",
    "                              shuffle=True,\n",
    "                              epochs = EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 22050)\n",
      "(2, 22050)\n",
      "(1, 22050)\n"
     ]
    }
   ],
   "source": [
    "# Second and simple method\n",
    "\n",
    "df = pd.read_csv('train_curated'+'.csv', encoding='utf-8')\n",
    "target = df.pop('labels')\n",
    "\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "label_names = []\n",
    "for row in list(set(target.values)):\n",
    "    label_names.extend(tokenizer.tokenize(row))\n",
    "label_names = list(set(label_names))\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(('train_curated/'+df.values, target.values))\n",
    "\n",
    "def DataPreproc(filename_tensor, labels):\n",
    "    text = filename_tensor.numpy()[0]\n",
    "    labels = labels.numpy()\n",
    "    y, sr = librosa.load(text, mono = True)\n",
    "\n",
    "    if y.shape[0] == 0:\n",
    "        y = np.zeros((sr), dtype=np.float32)\n",
    "    y.resize((int(y.shape[0]/sr), sr), refcheck=False)\n",
    "    \n",
    "    labels = tokenizer.tokenize(labels)\n",
    "    label = np.zeros_like(label_names, dtype=np.int32)\n",
    "    for lb in labels:\n",
    "        label[label_names.index(lb)] = 1\n",
    "        \n",
    "    return y, label\n",
    "def DataPreproc_map(filename_tensor, labels):\n",
    "    return tf.py_function(DataPreproc, inp = [filename_tensor, labels],\n",
    "                          Tout = (tf.float32, tf.int32))\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "BUFFER = 3000\n",
    "\n",
    "#train_data = train_data.shuffle(BUFFER, reshuffle_each_iteration=False)\n",
    "\n",
    "train_data = train_data.map(DataPreproc_map)\n",
    "\n",
    "test_data = train_data.skip(3200)\n",
    "valid_data = test_data.take(1000)\n",
    "test_data = test_data.skip(1000)\n",
    "train_data = train_data.take(3200)\n",
    "\n",
    "for ex in train_data.take(3):\n",
    "    print(ex[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.shuffle(BUFFER, reshuffle_each_iteration=False).padded_batch(BATCH_SIZE, padded_shapes=([None, None], [None]))\n",
    "valid_data = valid_data.shuffle(BUFFER, reshuffle_each_iteration=False).padded_batch(BATCH_SIZE, padded_shapes=([None, None], [None]))\n",
    "test_data = test_data.padded_batch(1, padded_shapes=([None, None], [None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidir-lstm (Bidirectional)   (None, 64)                5653248   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 86)                11094     \n",
      "=================================================================\n",
      "Total params: 5,672,662\n",
      "Trainable params: 5,672,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building LSTM 2 model\n",
    "\n",
    "ex = train_data.take(1)\n",
    "shape_1 = ex[0].shape[2]\n",
    "output_shape = ex[1].shape[1]\n",
    "\n",
    "bi_lstm_model = tf.keras.Sequential([\n",
    "    layers.Bidirectional(layers.LSTM(32, name = 'lstm-1'), input_shape=(None, shape_1),\n",
    "                                  name = 'bidir-lstm'),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(output_shape, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "bi_lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/107 [==============================] - 1051s 10s/step - loss: 5.5716 - accuracy: 0.0225 - val_loss: 5.6556 - val_accuracy: 0.0350\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 998s 9s/step - loss: 5.2922 - accuracy: 0.0919 - val_loss: 5.5316 - val_accuracy: 0.0400\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 1003s 9s/step - loss: 4.2209 - accuracy: 0.2419 - val_loss: 5.6851 - val_accuracy: 0.0450\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 1000s 9s/step - loss: 2.9891 - accuracy: 0.4647 - val_loss: 6.0829 - val_accuracy: 0.0490\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 1000s 9s/step - loss: 2.3166 - accuracy: 0.5797 - val_loss: 6.3532 - val_accuracy: 0.0530\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 986s 9s/step - loss: 1.9362 - accuracy: 0.6381 - val_loss: 6.6604 - val_accuracy: 0.0490\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 1032s 10s/step - loss: 1.7024 - accuracy: 0.6694 - val_loss: 6.9814 - val_accuracy: 0.0490\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 997s 9s/step - loss: 1.5340 - accuracy: 0.6947 - val_loss: 7.3817 - val_accuracy: 0.0470\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 1001s 9s/step - loss: 1.4285 - accuracy: 0.7078 - val_loss: 7.7076 - val_accuracy: 0.0550\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 1010s 9s/step - loss: 1.3451 - accuracy: 0.7206 - val_loss: 7.8629 - val_accuracy: 0.0450\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:941 test_function  *\n        outputs = self.distribute_strategy.run(\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:909 test_step  **\n        y_pred = self(x, training=False)\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:886 __call__\n        self.name)\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:168 assert_input_compatibility\n        layer_name + ' is incompatible with the layer: '\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-8ac1a283c159>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Evaluate on test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtest_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbi_lstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test acc.: {:.2f}%'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1079\u001b[0m                 step_num=step):\n\u001b[0;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    616\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2419\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2420\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2772\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2773\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 2774\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2704\u001b[0m         relaxed_arg_shapes)\n\u001b[0;32m   2705\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 2706\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   2707\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2667\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:941 test_function  *\n        outputs = self.distribute_strategy.run(\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:909 test_step  **\n        y_pred = self(x, training=False)\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:886 __call__\n        self.name)\n    D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:168 assert_input_compatibility\n        layer_name + ' is incompatible with the layer: '\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.\n"
     ]
    }
   ],
   "source": [
    "#Compile and train the model\n",
    "\n",
    "bi_lstm_model.compile(optimizer = tf.keras.optimizers.Adam(1e-3),\n",
    "                      loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                      metrics = ['accuracy'])\n",
    "history = bi_lstm_model.fit(train_data, \n",
    "                            validation_data = valid_data,\n",
    "                            epochs = EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "test_results = bi_lstm_model.evaluate(test_data)\n",
    "print('Test acc.: {:.2f}%'.format(test_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataConstruct(foldername):\n",
    "    DATA_simple = []\n",
    "    DATA_spectogram = []\n",
    "    LABELS = []\n",
    "    filenames = os.listdir(foldername)\n",
    "    df = pd.read_csv(foldername+'.csv', encoding='utf-8')\n",
    "        \n",
    "    for filename, labels in df.values:\n",
    "        text = foldername + '/' + filename\n",
    "        y, sr = librosa.load(text, mono = True)\n",
    "    \n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "\n",
    "        data = np.array((np.mean(chroma_stft), np.mean(spec_cent), \n",
    "                        np.mean(spec_bw), np.mean(rolloff), \n",
    "                        np.mean(zcr), np.mean(mfcc)), dtype = np.float64)\n",
    "        DATA.append(data)\n",
    "        labels = tokenizer.tokenize(labels)\n",
    "        label = np.zeros_like(label_names, dtype=np.int64)\n",
    "        for lb in labels:\n",
    "            label[label_names.index(lb)] = 1\n",
    "        LABELS.append(label)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    DATA = scaler.fit_transform(DATA)\n",
    "    \n",
    "    return DATA, LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\librosa\\core\\pitch.py:153: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn(\"Trying to estimate tuning from empty frequency set.\")\n"
     ]
    }
   ],
   "source": [
    "data, targets = DataConstruct('train_curated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((data, targets))\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "BUFFER = 5000\n",
    "\n",
    "train_data = train_data.shuffle(BUFFER, reshuffle_each_iteration=False)\n",
    "test_data = train_data.skip(3200)\n",
    "valid_data = test_data.take(1000).batch(BATCH_SIZE)\n",
    "test_data = test_data.skip(1000).batch(BATCH_SIZE)\n",
    "train_data = train_data.take(3200).batch(BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train example shapes |  (3, 6)\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "fc_1 (Dense)                 (None, 256)               1792      \n",
      "_________________________________________________________________\n",
      "fc_2 (Dense)                 (None, 528)               135696    \n",
      "_________________________________________________________________\n",
      "fc_3 (Dense)                 (None, 528)               279312    \n",
      "_________________________________________________________________\n",
      "fc_4 (Dense)                 (None, 528)               279312    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 528)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               135424    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 86)                22102     \n",
      "=================================================================\n",
      "Total params: 853,638\n",
      "Trainable params: 853,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.4772 - accuracy: 0.0178 - val_loss: 5.4052 - val_accuracy: 0.0240\n",
      "Epoch 2/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.4094 - accuracy: 0.0200 - val_loss: 5.3538 - val_accuracy: 0.0210\n",
      "Epoch 3/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.4723 - accuracy: 0.0216 - val_loss: 5.4751 - val_accuracy: 0.0110\n",
      "Epoch 4/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.5620 - accuracy: 0.0156 - val_loss: 5.5676 - val_accuracy: 0.0180\n",
      "Epoch 5/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6046 - accuracy: 0.0125 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 6/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6192 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 7/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6191 - accuracy: 0.0137 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 8/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 9/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 10/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 11/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 12/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 13/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 14/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 15/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 16/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 17/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 18/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 19/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 20/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 21/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 22/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 23/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 24/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 25/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 26/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 27/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 28/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 29/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 30/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 31/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 32/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 33/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 34/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 35/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 36/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 37/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 38/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 39/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 40/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 41/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 42/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 43/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 44/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 45/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 46/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 47/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 48/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 49/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 50/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 51/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 52/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 53/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 54/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 55/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 56/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 57/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 58/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 59/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 60/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 61/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 62/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 63/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 64/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 65/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 66/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 67/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 68/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 69/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 70/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 71/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 72/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 73/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 74/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 75/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 76/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 77/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 78/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 79/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 80/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 81/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 82/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 83/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 84/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 85/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 86/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 87/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 88/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 89/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 90/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 91/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 92/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 93/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 94/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 95/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 96/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 97/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 98/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 99/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 100/150\n",
      "1067/1067 [==============================] - 5s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 101/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 102/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 103/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 104/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 105/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 106/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 107/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 108/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 109/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 110/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 111/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 112/150\n",
      "1067/1067 [==============================] - 5s 5ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 113/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 114/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 115/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 116/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 117/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 118/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 119/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 120/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 121/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 122/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 123/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 124/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 125/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 126/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 127/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 128/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 129/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 130/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 131/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 132/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 133/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 134/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 135/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 136/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 137/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 138/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 139/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 140/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 141/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 142/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 143/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 144/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 145/150\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 146/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 147/150\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 148/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 149/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "Epoch 150/150\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 5.6194 - accuracy: 0.0134 - val_loss: 5.5894 - val_accuracy: 0.0100\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 5.5672 - accuracy: 0.0195\n",
      "Test acc:  1.9480518996715546 %\n"
     ]
    }
   ],
   "source": [
    "# Building 3_model\n",
    "# OverFIT\n",
    "\n",
    "for ex in train_data.take(1):\n",
    "    output_shape = ex[1].shape[1]\n",
    "    input_shape = ex[0].shape[1]\n",
    "    print('train example shapes | ', ex[0].shape)\n",
    "\n",
    "inputs = keras.Input(shape=(input_shape))\n",
    "\n",
    "x = layers.Dense(128, activation='relu', name='fc_1')(inputs)\n",
    "x = layers.Dense(256, activation='relu', name='fc_2')(x)\n",
    "x = layers.Dense(256, activation='relu', name='fc_3')(x)\n",
    "\n",
    "y = layers.Dropout(0.2)(x)\n",
    "z = layers.Dense(256)(y)\n",
    "\n",
    "output = layers.Dense(output_shape, activation='sigmoid', name='output')(z)\n",
    "\n",
    "model_1 = keras.Model(inputs = inputs, outputs = output)\n",
    "model_1.summary()\n",
    "\n",
    "model_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "hist_1 = model_1.fit(train_data, validation_data=valid_data, epochs = 150, shuffle=True)\n",
    "\n",
    "test_res_1 = model_1.evaluate(test_data)\n",
    "\n",
    "# OVERFIT\n",
    "print('Test acc: ', test_res_1[1]*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
